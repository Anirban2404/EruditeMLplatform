,{
"cell_type": "code",
"execution_count": 1,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["import pandas as pd\nimport numpy as np\n\n# vanilla ML methods ------------------------------------------------\n# linear algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC, SVC\n# metric algorithms\nfrom sklearn.neighbors import KNeighborsClassifier\n# ensemble algorithms\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n# deep learning -----------------------------------------------------\n\n# visualize results / dimensionality reduction / manifold learning (TODO)\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# utilities / preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\n\nimport pandas as pd\nimport numpy as np\ntrain = pd.read_csv('../Datasets/MNIST_dataset/train.csv', engine='c', sep=',')\ntest = pd.read_csv('../Datasets/MNIST_dataset/test.csv', engine='c', sep=',')\nprint(train.shape, test.shape)\ntrain.head()\n# extract X, y from data\nX, y = train.drop('label', axis=1).values, train['label'].values"]
    }
,{
"cell_type": "code",
"execution_count": 2,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["# visualize pictures\npicture_size = (28, 28)\nexamples = [train[train.label == k].sample(1, random_state=42).values for k in range(10)]\n\nf, axarr = plt.subplots(1, 10, squeeze=False, figsize=(12, 1.2))\n\nfor i,e in enumerate(examples):\n    # reshape 1D to 2D array - a picture\n    img = e[:, 1:].reshape(picture_size).astype(float)\n    # then draw it\n    axarr[0, i].set_title(i)\n    axarr[0, i].imshow(img, cmap='gray', interpolation='bicubic')"]
    }
,{
"cell_type": "code",
"execution_count": 3,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["# utilities / preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\n# Scale features to [0,1], some models are sensitive to non-scaled data\nsc = MinMaxScaler()\nX = sc.fit_transform(X)"]
    }
,{
"cell_type": "code",
"execution_count": 4,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["X_train, X_holdout, y_train, y_holdout = train_test_split(\n                                            X,\n                                            y,\n                                            test_size=0.15,\n                                            random_state=42,\n                                            # to preserve initial class balance\n                                            stratify=train[train.columns[0]].values, \n                                                         )"]
    }
,{
"cell_type": "code",
"execution_count": 5,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["X_train, X_holdout, y_train, y_holdout = train_test_split(\n                                            X,\n                                            y,\n                                            test_size=0.15,\n                                            random_state=42,\n                                            # to preserve initial class balance\n                                            stratify=train[train.columns[0]].values, \n                                                         )"]
    }
,{
"cell_type": "code",
"execution_count": 6,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["%%time\n# KNN\nclf = KNeighborsClassifier(n_jobs=-1, \n                           n_neighbors=10,\n                           weights='distance', \n                           p=2\n                          )\n\n\nclf.fit(X_train, y_train)\nacc = accuracy_score(clf.predict(X_holdout), y_holdout)\nprint(acc)"]
    }
,{
"cell_type": "code",
"execution_count": 7,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["%%time\n# KNN on PCA vectors - faster + better accuracy\npca = PCA(n_components=40, random_state=42)\nX_transformed = pca.fit_transform(X_train)\n\n\nclf.fit(X_transformed, y_train)\nacc = accuracy_score(clf.predict(pca.transform(X_holdout)), y_holdout)\nprint(acc)"]
    }
,{
"cell_type": "code",
"execution_count": 8,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["%%time\n# Logistic regression\nclf = LogisticRegression(n_jobs=-1, \n                         C=1000,\n                         fit_intercept=False,\n                         class_weight='balanced', \n                         random_state=42,\n                         solver='lbfgs',\n                         multi_class='multinomial',\n                         verbose=3)\n\n\n\nclf.fit(X_train, y_train)\nacc = accuracy_score(clf.predict(X_holdout), y_holdout)\nprint(acc)"]
    }
,{
"cell_type": "code",
"execution_count": 9,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["%%time\n# Logistic regression on PCA vectors - faster, higher accuracy\npca = PCA(n_components=150, random_state=42)\nX_transformed = pca.fit_transform(X_train)\n\n\nclf.fit(X_transformed, y_train)\nacc = accuracy_score(clf.predict(pca.transform(X_holdout)), y_holdout)\nprint(acc)\n"]
    }
,{
"cell_type": "code",
"execution_count": 10,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["%%time\n# Linear SVC\nclf = LinearSVC(C=0.05, \n                fit_intercept=True, \n                class_weight='balanced',\n                multi_class='crammer_singer',\n                dual=False,\n                random_state=42)\n\n\nclf.fit(X_train, y_train)\nacc = accuracy_score(clf.predict(X_holdout), y_holdout)\nprint(acc)"]
    }
,{
"cell_type": "code",
"execution_count": 11,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["%%time\n# SVC on PCA vectors - slower, and lower accuracy\npca = PCA(n_components=150, random_state=42)\nX_transformed = pca.fit_transform(X_train)\n\n\nclf.fit(X_transformed, y_train)\nacc = accuracy_score(clf.predict(pca.transform(X_holdout)), y_holdout)\nprint(acc)"]
    }
,{
"cell_type": "code",
"execution_count": 12,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["%%time\n# SVM with non-linear kernel\nclf = SVC(C=12,\n          kernel='rbf',\n          class_weight='balanced',\n          random_state=42\n         )\n\nclf.fit(X_train, y_train)\nacc = accuracy_score(clf.predict(X_holdout), y_holdout)\nprint(acc)"]
    }
,{
"cell_type": "code",
"execution_count": 13,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["pca = PCA(n_components=40, random_state=42)\nX_transformed = pca.fit_transform(X_train)\n\n\nclf.fit(X_transformed, y_train)\nacc = accuracy_score(clf.predict(pca.transform(X_holdout)), y_holdout)\nprint(acc)"]
    }
,{
"cell_type": "code",
"execution_count": 14,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": [undefined]
    }
,{
"cell_type": "code",
"execution_count": 15,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["%%time\n# Random Forest\nclf = RandomForestClassifier(n_estimators=250, \n                             n_jobs=-1, \n                             random_state=42, \n                             class_weight='balanced',\n                             criterion='gini'\n                             \n)\n\n\nclf.fit(X_train, y_train)\nacc = accuracy_score(clf.predict(X_holdout), y_holdout)\nprint(acc)"]
    }
,{
"cell_type": "code",
"execution_count": 16,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": [undefined]
    }
,{
"cell_type": "code",
"execution_count": 17,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["%%time\n# Gradient Boosting\nclf = GradientBoostingClassifier(n_estimators=90, \n                                 random_state=42, \n                                 verbose=0,\n                                 max_features='sqrt',\n                                 subsample=0.7,\n                                 learning_rate=0.1, \n                                 max_depth=5\n                                \n)\n\nclf.fit(X_train, y_train)\nacc = accuracy_score(clf.predict(X_holdout), y_holdout)\nprint(acc)"]
    }
,{
"cell_type": "code",
"execution_count": 18,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": [undefined]
    }
,{
"cell_type": "code",
"execution_count": 19,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": [undefined]
    }
,{
"cell_type": "code",
"execution_count": 20,
"metadata": {
"collapsed": true
},
"outputs": [],
"source": ["%%time\n\n# now use the whole dataset to train\nclf.fit(X, y) \n\n# prediction\ny_pred = clf.predict(test.values)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfig=plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\nfor i, pred in enumerate(y_pred[5000:5050]):\n    fig.add_subplot(5,10,i+1)\n    plt.subplots_adjust(hspace=.1)\n    plt.imshow(test.values[pred].reshape(28,28), cmap='gray', interpolation='none')\n    #plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], test_y[correct]))\n    plt.title(\"Predicted {}\".format(y_pred[pred]))"]
    }
